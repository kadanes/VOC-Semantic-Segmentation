Model Architecture to be Trained: 
FCN_resnet_bn_skp(
  (pool3): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (pool4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (pool5): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (upconv4): Sequential(
    (0): ReLU()
    (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (upconv3): Sequential(
    (0): ReLU()
    (1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (upconv): Sequential(
    (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(32, 21, kernel_size=(1, 1), stride=(1, 1))
  )
)
~~~~~~~~~~~~~~~
loading ./voc2012_train.h5
loading ./voc2012_val.h5
~~~~~~~~~~~~~~~
Epoch:1, Loss:0.8483 Val set: Average loss: 0.0130, Accuracy: 5742/50176 (11%)
Epoch:2, Loss:0.5953 Val set: Average loss: 0.0123, Accuracy: 36417/50176 (73%)
Epoch:3, Loss:0.5667 Val set: Average loss: 0.0116, Accuracy: 37101/50176 (74%)
Epoch:4, Loss:0.5204 Val set: Average loss: 0.0115, Accuracy: 37262/50176 (74%)
Epoch:5, Loss:0.4988 Val set: Average loss: 0.0116, Accuracy: 32859/50176 (65%)
Epoch:6, Loss:0.3826 Val set: Average loss: 0.0114, Accuracy: 36580/50176 (73%)
Epoch:7, Loss:0.3630 Val set: Average loss: 0.0109, Accuracy: 33600/50176 (67%)
Epoch:8, Loss:0.3580 Val set: Average loss: 0.0107, Accuracy: 36802/50176 (73%)
Epoch:9, Loss:0.3198 Val set: Average loss: 0.0107, Accuracy: 40147/50176 (80%)
Epoch:10, Loss:0.2915 Val set: Average loss: 0.0105, Accuracy: 40325/50176 (80%)
Epoch:11, Loss:0.2474 Val set: Average loss: 0.0108, Accuracy: 40451/50176 (81%)
Epoch:12, Loss:0.2339 Val set: Average loss: 0.0107, Accuracy: 41061/50176 (82%)
Epoch:13, Loss:0.3227 Val set: Average loss: 0.0102, Accuracy: 40829/50176 (81%)
Epoch:14, Loss:0.2741 Val set: Average loss: 0.0104, Accuracy: 40231/50176 (80%)
Epoch:15, Loss:0.2509 Val set: Average loss: 0.0104, Accuracy: 39745/50176 (79%)
Epoch:16, Loss:0.2298 Val set: Average loss: 0.0104, Accuracy: 39604/50176 (79%)
Epoch:17, Loss:0.2193 Val set: Average loss: 0.0100, Accuracy: 40627/50176 (81%)
Epoch:18, Loss:0.2632 Val set: Average loss: 0.0100, Accuracy: 41185/50176 (82%)
Epoch:19, Loss:0.2551 Val set: Average loss: 0.0102, Accuracy: 41191/50176 (82%)
Epoch:20, Loss:0.2392 Val set: Average loss: 0.0101, Accuracy: 41171/50176 (82%)
Epoch:21, Loss:0.2286 Val set: Average loss: 0.0107, Accuracy: 39904/50176 (80%)
Epoch:22, Loss:0.2163 Val set: Average loss: 0.0101, Accuracy: 41215/50176 (82%)
Epoch:23, Loss:0.1778 Val set: Average loss: 0.0100, Accuracy: 41753/50176 (83%)
Epoch:24, Loss:0.1836 Val set: Average loss: 0.0104, Accuracy: 41459/50176 (83%)
Epoch:25, Loss:0.1746 Val set: Average loss: 0.0107, Accuracy: 41168/50176 (82%)
Epoch:26, Loss:0.1654 Val set: Average loss: 0.0104, Accuracy: 41985/50176 (84%)
Epoch:27, Loss:0.1612 Val set: Average loss: 0.0106, Accuracy: 41667/50176 (83%)
Epoch:28, Loss:0.1665 Val set: Average loss: 0.0104, Accuracy: 41899/50176 (84%)
Epoch:29, Loss:0.1513 Val set: Average loss: 0.0102, Accuracy: 42105/50176 (84%)
Epoch:30, Loss:0.1485 Val set: Average loss: 0.0105, Accuracy: 41519/50176 (83%)
Epoch:31, Loss:0.1410 Val set: Average loss: 0.0104, Accuracy: 41585/50176 (83%)
Epoch:32, Loss:0.2156 Val set: Average loss: 0.0108, Accuracy: 41044/50176 (82%)
Epoch:33, Loss:0.1454 Val set: Average loss: 0.0104, Accuracy: 41445/50176 (83%)
Epoch:34, Loss:0.1484 Val set: Average loss: 0.0105, Accuracy: 41770/50176 (83%)
Epoch:35, Loss:0.1319 Val set: Average loss: 0.0102, Accuracy: 42010/50176 (84%)
Epoch:36, Loss:0.1542 Val set: Average loss: 0.0108, Accuracy: 41559/50176 (83%)
Epoch:37, Loss:0.1290 Val set: Average loss: 0.0104, Accuracy: 41874/50176 (83%)
Epoch:38, Loss:0.1317 Val set: Average loss: 0.0102, Accuracy: 42059/50176 (84%)
Epoch:39, Loss:0.1250 Val set: Average loss: 0.0101, Accuracy: 42054/50176 (84%)
Epoch:40, Loss:0.1347 Val set: Average loss: 0.0103, Accuracy: 41742/50176 (83%)
Epoch:41, Loss:0.1284 Val set: Average loss: 0.0105, Accuracy: 42025/50176 (84%)
Epoch:42, Loss:0.1169 Val set: Average loss: 0.0107, Accuracy: 41647/50176 (83%)
Epoch:43, Loss:0.1206 Val set: Average loss: 0.0102, Accuracy: 41927/50176 (84%)
Epoch:44, Loss:0.1190 Val set: Average loss: 0.0104, Accuracy: 41876/50176 (83%)
Epoch:45, Loss:0.1279 Val set: Average loss: 0.0104, Accuracy: 41835/50176 (83%)
Epoch:46, Loss:0.1291 Val set: Average loss: 0.0107, Accuracy: 41871/50176 (83%)
Epoch:47, Loss:0.1338 Val set: Average loss: 0.0107, Accuracy: 41819/50176 (83%)
Epoch:48, Loss:0.1148 Val set: Average loss: 0.0106, Accuracy: 41697/50176 (83%)
Epoch:49, Loss:0.1263 Val set: Average loss: 0.0109, Accuracy: 41346/50176 (82%)
Epoch:50, Loss:0.1172 Val set: Average loss: 0.0104, Accuracy: 41785/50176 (83%)
Epoch:51, Loss:0.1151 Val set: Average loss: 0.0106, Accuracy: 41177/50176 (82%)
Epoch:52, Loss:0.1136 Val set: Average loss: 0.0103, Accuracy: 41951/50176 (84%)
Epoch:53, Loss:0.1154 Val set: Average loss: 0.0106, Accuracy: 41827/50176 (83%)
Epoch:54, Loss:0.1108 Val set: Average loss: 0.0105, Accuracy: 41831/50176 (83%)
Epoch:55, Loss:0.1072 Val set: Average loss: 0.0104, Accuracy: 41867/50176 (83%)
Epoch:56, Loss:0.1104 Val set: Average loss: 0.0101, Accuracy: 41988/50176 (84%)
Epoch:57, Loss:0.1137 Val set: Average loss: 0.0103, Accuracy: 41751/50176 (83%)
Epoch:58, Loss:0.1049 Val set: Average loss: 0.0103, Accuracy: 41787/50176 (83%)
Epoch:59, Loss:0.1074 Val set: Average loss: 0.0105, Accuracy: 41914/50176 (84%)
Epoch:60, Loss:0.1095 Val set: Average loss: 0.0107, Accuracy: 41816/50176 (83%)
Epoch:61, Loss:0.1153 Val set: Average loss: 0.0106, Accuracy: 41660/50176 (83%)
Epoch:62, Loss:0.1125 Val set: Average loss: 0.0104, Accuracy: 41845/50176 (83%)
Epoch:63, Loss:0.1098 Val set: Average loss: 0.0106, Accuracy: 41915/50176 (84%)
Epoch:64, Loss:0.1035 Val set: Average loss: 0.0105, Accuracy: 41976/50176 (84%)
Epoch:65, Loss:0.1043 Val set: Average loss: 0.0108, Accuracy: 41593/50176 (83%)
Epoch:66, Loss:0.1033 Val set: Average loss: 0.0105, Accuracy: 41893/50176 (83%)
Epoch:67, Loss:0.1025 Val set: Average loss: 0.0108, Accuracy: 41520/50176 (83%)
Epoch:68, Loss:0.1081 Val set: Average loss: 0.0106, Accuracy: 41833/50176 (83%)
Epoch:69, Loss:0.1276 Val set: Average loss: 0.0112, Accuracy: 40994/50176 (82%)
Epoch:70, Loss:0.1119 Val set: Average loss: 0.0107, Accuracy: 41537/50176 (83%)
Epoch:71, Loss:0.1101 Val set: Average loss: 0.0110, Accuracy: 41301/50176 (82%)
Epoch:72, Loss:0.1064 Val set: Average loss: 0.0111, Accuracy: 41154/50176 (82%)
Epoch:73, Loss:0.1057 Val set: Average loss: 0.0108, Accuracy: 41243/50176 (82%)
Epoch:74, Loss:0.1090 Val set: Average loss: 0.0105, Accuracy: 41720/50176 (83%)
Epoch:75, Loss:0.1022 Val set: Average loss: 0.0104, Accuracy: 41913/50176 (84%)
Epoch:76, Loss:0.1021 Val set: Average loss: 0.0107, Accuracy: 41801/50176 (83%)
Epoch:77, Loss:0.0974 Val set: Average loss: 0.0107, Accuracy: 41657/50176 (83%)
Epoch:78, Loss:0.0988 Val set: Average loss: 0.0106, Accuracy: 41702/50176 (83%)
Epoch:79, Loss:0.0946 Val set: Average loss: 0.0106, Accuracy: 41759/50176 (83%)
Epoch:80, Loss:0.1019 Val set: Average loss: 0.0109, Accuracy: 41357/50176 (82%)
Epoch:81, Loss:0.1042 Val set: Average loss: 0.0105, Accuracy: 41718/50176 (83%)
Epoch:82, Loss:0.1045 Val set: Average loss: 0.0105, Accuracy: 41429/50176 (83%)
Epoch:83, Loss:0.1113 Val set: Average loss: 0.0105, Accuracy: 41285/50176 (82%)
Epoch:84, Loss:0.1046 Val set: Average loss: 0.0105, Accuracy: 41341/50176 (82%)
Epoch:85, Loss:0.1099 Val set: Average loss: 0.0111, Accuracy: 41005/50176 (82%)
Epoch:86, Loss:0.1128 Val set: Average loss: 0.0106, Accuracy: 41132/50176 (82%)
Epoch:87, Loss:0.1060 Val set: Average loss: 0.0106, Accuracy: 41728/50176 (83%)
Epoch:88, Loss:0.0965 Val set: Average loss: 0.0106, Accuracy: 41821/50176 (83%)
Epoch:89, Loss:0.0987 Val set: Average loss: 0.0103, Accuracy: 41819/50176 (83%)
Epoch:90, Loss:0.0994 Val set: Average loss: 0.0103, Accuracy: 41815/50176 (83%)
Epoch:91, Loss:0.0964 Val set: Average loss: 0.0103, Accuracy: 41704/50176 (83%)
Epoch:92, Loss:0.0938 Val set: Average loss: 0.0104, Accuracy: 41675/50176 (83%)
Epoch:93, Loss:0.0975 Val set: Average loss: 0.0104, Accuracy: 41900/50176 (84%)
Epoch:94, Loss:0.1029 Val set: Average loss: 0.0109, Accuracy: 41626/50176 (83%)
Epoch:95, Loss:0.0987 Val set: Average loss: 0.0110, Accuracy: 41418/50176 (83%)
Epoch:96, Loss:0.0967 Val set: Average loss: 0.0108, Accuracy: 41430/50176 (83%)
Epoch:97, Loss:0.1037 Val set: Average loss: 0.0104, Accuracy: 41672/50176 (83%)
Epoch:98, Loss:0.1021 Val set: Average loss: 0.0104, Accuracy: 41913/50176 (84%)
Epoch:99, Loss:0.0995 Val set: Average loss: 0.0106, Accuracy: 41672/50176 (83%)
Epoch:100, Loss:0.0994 Val set: Average loss: 0.0107, Accuracy: 41672/50176 (83%)
Training time:  446352:46:41.21