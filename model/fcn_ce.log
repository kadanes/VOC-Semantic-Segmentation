FCN(
  (vgg16): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (flattent): Sequential()
  (upconv): Sequential(
    (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout2d(p=0.5, inplace=False)
    (3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (4): LeakyReLU(negative_slope=0.01)
    (5): Dropout2d(p=0.5, inplace=False)
    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (7): LeakyReLU(negative_slope=0.01)
    (8): Dropout2d(p=0.5, inplace=False)
    (9): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (10): LeakyReLU(negative_slope=0.01)
    (11): Dropout2d(p=0.5, inplace=False)
    (12): ConvTranspose2d(32, 21, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
  )
  (fcn16): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01)
      (2): Dropout2d(p=0.5, inplace=False)
      (3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (4): LeakyReLU(negative_slope=0.01)
      (5): Dropout2d(p=0.5, inplace=False)
      (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (7): LeakyReLU(negative_slope=0.01)
      (8): Dropout2d(p=0.5, inplace=False)
      (9): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (10): LeakyReLU(negative_slope=0.01)
      (11): Dropout2d(p=0.5, inplace=False)
      (12): ConvTranspose2d(32, 21, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    )
  )
)
Criterion: CrossEntropyLoss()
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)
Learning Rate: 0.001
Weight Decay: 1e-05
Batch Size: 64
Epochs: 200
-----------------------
Epoch:1, Loss:2.0523 
Epoch:2, Loss:1.6891 
Epoch:3, Loss:1.5154 
Epoch:4, Loss:1.4477 
Epoch:5, Loss:1.4107 
Epoch:6, Loss:1.4481 
Epoch:7, Loss:1.3579 
Epoch:8, Loss:1.3207 
Epoch:9, Loss:1.3451 
Epoch:10, Loss:1.2956 
Epoch:11, Loss:1.2603 
Epoch:12, Loss:1.2936 
Epoch:13, Loss:1.2174 
Epoch:14, Loss:1.2424 
Epoch:15, Loss:1.1733 
Epoch:16, Loss:1.2183 
Epoch:17, Loss:1.1971 
Epoch:18, Loss:1.1948 
Epoch:19, Loss:1.1181 
Epoch:20, Loss:1.1586 
Epoch:21, Loss:1.1824 
Epoch:22, Loss:1.1194 
Epoch:23, Loss:1.0844 
Epoch:24, Loss:1.0639 
Epoch:25, Loss:1.0823 
Epoch:26, Loss:1.0677 
Epoch:27, Loss:1.0606 
Epoch:28, Loss:1.0598 
Epoch:29, Loss:1.0492 
Epoch:30, Loss:1.0284 
Epoch:31, Loss:1.0161 
Epoch:32, Loss:0.9768 
Epoch:33, Loss:0.9823 
Epoch:34, Loss:0.9964 
Epoch:35, Loss:0.9860 
Epoch:36, Loss:1.0214 
Epoch:37, Loss:1.0546 
Epoch:38, Loss:1.0088 
Epoch:39, Loss:0.9685 
Epoch:40, Loss:0.9790 
Epoch:41, Loss:0.9985 
Epoch:42, Loss:0.9251 
Epoch:43, Loss:0.8965 
Epoch:44, Loss:0.8839 
Epoch:45, Loss:0.8848 
Epoch:46, Loss:0.8904 
Epoch:47, Loss:0.8542 
Epoch:48, Loss:0.8835 
Epoch:49, Loss:0.8637 
Epoch:50, Loss:0.8594 
Epoch:51, Loss:0.8472 
Epoch:52, Loss:0.8415 
Epoch:53, Loss:0.8210 
Epoch:54, Loss:0.8321 
Epoch:55, Loss:0.8268 
Epoch:56, Loss:0.8306 
Epoch:57, Loss:0.8175 
Epoch:58, Loss:0.8018 
Epoch:59, Loss:0.8164 
Epoch:60, Loss:0.8168 
Epoch:61, Loss:0.8160 
Epoch:62, Loss:0.8308 
Epoch:63, Loss:0.7946 
Epoch:64, Loss:0.7837 
Epoch:65, Loss:0.7688 
Epoch:66, Loss:0.7557 
Epoch:67, Loss:0.7626 
Epoch:68, Loss:0.7716 
Epoch:69, Loss:0.7532 
Epoch:70, Loss:0.7571 
Epoch:71, Loss:0.7870 
Epoch:72, Loss:0.8315 
Epoch:73, Loss:0.7517 
Epoch:74, Loss:0.7606 
Epoch:75, Loss:0.7906 
Epoch:76, Loss:0.7806 
Epoch:77, Loss:0.7590 
Epoch:78, Loss:0.7209 
Epoch:79, Loss:0.7226 
Epoch:80, Loss:0.7072 
Epoch:81, Loss:0.9891 
Epoch:82, Loss:0.9362 
Epoch:83, Loss:0.8431 
Epoch:84, Loss:0.7565 
Epoch:85, Loss:0.7243 
Epoch:86, Loss:0.6727 
Epoch:87, Loss:0.6851 
Epoch:88, Loss:0.6868 
Epoch:89, Loss:0.6914 
Epoch:90, Loss:0.7315 
Epoch:91, Loss:0.7023 
Epoch:92, Loss:0.6851 
Epoch:93, Loss:0.6369 
Epoch:94, Loss:0.6277 
Epoch:95, Loss:0.6391 
Epoch:96, Loss:0.6836 
Epoch:97, Loss:0.6714 
Epoch:98, Loss:0.6261 
Epoch:99, Loss:0.6017 
Epoch:100, Loss:0.5942 
Epoch:101, Loss:0.5859 
Epoch:102, Loss:0.6038 
Epoch:103, Loss:0.5843 
Epoch:104, Loss:0.5950 
Epoch:105, Loss:0.5765 
Epoch:106, Loss:0.5634 
Epoch:107, Loss:0.5926 
Epoch:108, Loss:0.5615 
Epoch:109, Loss:0.5483 
Epoch:110, Loss:0.5527 
Epoch:111, Loss:0.5570 
Epoch:112, Loss:0.5625 
Epoch:113, Loss:0.5597 
Epoch:114, Loss:0.6161 
Epoch:115, Loss:0.5936 
Epoch:116, Loss:0.5517 
Epoch:117, Loss:0.5768 
Epoch:118, Loss:0.5430 
Epoch:119, Loss:0.5618 
Epoch:120, Loss:0.6060 
Epoch:121, Loss:0.5749 
Epoch:122, Loss:0.5519 
Epoch:123, Loss:0.5490 
Epoch:124, Loss:0.5256 
Epoch:125, Loss:0.5201 
Epoch:126, Loss:0.5190 
Epoch:127, Loss:0.5325 
Epoch:128, Loss:0.5217 
Epoch:129, Loss:0.5409 
Epoch:130, Loss:0.5396 
Epoch:131, Loss:0.5695 
Epoch:132, Loss:0.5869 
Epoch:133, Loss:0.5638 
Epoch:134, Loss:0.5876 
Epoch:135, Loss:0.5666 
Epoch:136, Loss:0.5295 
Epoch:137, Loss:0.5373 
Epoch:138, Loss:0.5386 
Epoch:139, Loss:0.5031 
Epoch:140, Loss:0.4895 
Epoch:141, Loss:0.4971 
Epoch:142, Loss:0.4688 
Epoch:143, Loss:0.4806 
Epoch:144, Loss:0.4972 
Epoch:145, Loss:0.4384 
Epoch:146, Loss:0.4908 
Epoch:147, Loss:0.4951 
Epoch:148, Loss:0.4378 
Epoch:149, Loss:0.5360 
Epoch:150, Loss:0.5156 
Epoch:151, Loss:0.4805 
Epoch:152, Loss:0.4387 
Epoch:153, Loss:0.4405 
Epoch:154, Loss:0.4568 
Epoch:155, Loss:0.4315 
Epoch:156, Loss:0.4709 
Epoch:157, Loss:0.4493 
Epoch:158, Loss:0.6021 
Epoch:159, Loss:0.5495 
Epoch:160, Loss:0.5665 
Epoch:161, Loss:0.5595 
Epoch:162, Loss:0.5285 
Epoch:163, Loss:0.4743 
Epoch:164, Loss:0.4807 
Epoch:165, Loss:0.4458 
Epoch:166, Loss:0.4230 
Epoch:167, Loss:0.5152 
Epoch:168, Loss:0.4926 
Epoch:169, Loss:0.4560 
Epoch:170, Loss:0.4422 
Epoch:171, Loss:0.4198 
Epoch:172, Loss:0.4157 
Epoch:173, Loss:0.4095 
Epoch:174, Loss:0.4017 
Epoch:175, Loss:0.3905 
Epoch:176, Loss:0.4154 
Epoch:177, Loss:0.3849 
Epoch:178, Loss:0.4040 
Epoch:179, Loss:0.5268 
Epoch:180, Loss:0.4105 
Epoch:181, Loss:0.4070 
Epoch:182, Loss:0.3807 
Epoch:183, Loss:0.4135 
Epoch:184, Loss:0.3901 
Epoch:185, Loss:0.4178 
Epoch:186, Loss:0.4120 
Epoch:187, Loss:0.3886 
Epoch:188, Loss:0.3815 
Epoch:189, Loss:0.3720 
Epoch:190, Loss:0.3937 
Epoch:191, Loss:0.4084 
Epoch:192, Loss:0.3680 
Epoch:193, Loss:0.4248 
Epoch:194, Loss:0.3627 
Epoch:195, Loss:0.3530 
Epoch:196, Loss:0.3440 
Epoch:197, Loss:0.3668 
Epoch:198, Loss:0.3433 
Epoch:199, Loss:0.3473 
Epoch:200, Loss:0.3671 
